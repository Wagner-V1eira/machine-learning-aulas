{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bedcbad5",
   "metadata": {},
   "source": [
    "# M√©tricas de Avalia√ß√£o\n",
    "\n",
    "Este notebook apresenta as principais m√©tricas para avalia√ß√£o de modelos de classifica√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86663c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports b√°sicos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "import seaborn as sns\n",
    "\n",
    "# Configura√ß√£o de plotting\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c23a47",
   "metadata": {},
   "source": [
    "## Objetivos da Aula\n",
    "\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- Confusion Matrix\n",
    "- ROC Curve e AUC\n",
    "- Escolha da m√©trica adequada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea38e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria√ß√£o de dataset sint√©tico para demonstra√ß√£o\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "# Split dos dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Dataset criado:\")\n",
    "print(f\"- Treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"- Teste: {X_test.shape[0]} amostras\")\n",
    "print(f\"- Distribui√ß√£o treino: {np.bincount(y_train)}\")\n",
    "print(f\"- Distribui√ß√£o teste: {np.bincount(y_test)}\")\n",
    "\n",
    "# Treinamento de um modelo para demonstra√ß√£o\n",
    "clf = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predi√ß√µes\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]  # Probabilidade da classe positiva\n",
    "\n",
    "print(\"\\n‚úÖ Modelo treinado e predi√ß√µes realizadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6432dd",
   "metadata": {},
   "source": [
    "## 1. M√©tricas B√°sicas de Classifica√ß√£o\n",
    "\n",
    "### Accuracy (Acur√°cia)\n",
    "\n",
    "A **acur√°cia** √© a m√©trica mais simples e intuitiva: propor√ß√£o de predi√ß√µes corretas.\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{Predi√ß√µes Corretas}}{\\text{Total de Predi√ß√µes}}$$\n",
    "\n",
    "√â √∫til quando as classes est√£o balanceadas, mas pode ser enganosa em casos de desbalanceamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303043a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando m√©tricas b√°sicas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"üìä M√âTRICAS DE CLASSIFICA√á√ÉO\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1-Score:  {f1:.3f}\")\n",
    "\n",
    "# Visualiza√ß√£o das m√©tricas\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "values = [accuracy, precision, recall, f1]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(metrics, values, color=[\"skyblue\", \"lightgreen\", \"coral\", \"gold\"])\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"M√©tricas de Classifica√ß√£o\")\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01, f\"{value:.3f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a23df",
   "metadata": {},
   "source": [
    "### Precision, Recall e F1-Score\n",
    "\n",
    "Para entender essas m√©tricas, precisamos dos conceitos de **True Positives (TP)**, **False Positives (FP)**, **True Negatives (TN)** e **False Negatives (FN)**.\n",
    "\n",
    "**Precision (Precis√£o)**: Das predi√ß√µes positivas, quantas estavam corretas?\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**Recall (Revoca√ß√£o/Sensibilidade)**: Dos casos positivos reais, quantos foram identificados?\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**F1-Score**: M√©dia harm√¥nica entre Precision e Recall\n",
    "$$\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601e95c",
   "metadata": {},
   "source": [
    "## 2. Confusion Matrix (Matriz de Confus√£o)\n",
    "\n",
    "A **matriz de confus√£o** √© uma tabela que mostra as predi√ß√µes corretas e incorretas para cada classe, permitindo uma an√°lise detalhada dos erros do modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c462647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando e visualizando a matriz de confus√£o\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plotagem da matriz de confus√£o\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Classe 0\", \"Classe 1\"], yticklabels=[\"Classe 0\", \"Classe 1\"]\n",
    ")\n",
    "plt.title(\"Matriz de Confus√£o\")\n",
    "plt.xlabel(\"Predi√ß√£o\")\n",
    "plt.ylabel(\"Valor Real\")\n",
    "plt.show()\n",
    "\n",
    "# Extraindo valores da matriz\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"üìà COMPONENTES DA MATRIZ DE CONFUS√ÉO\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"True Negatives (TN):  {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"True Positives (TP):  {tp}\")\n",
    "\n",
    "print(\"\\nüìä C√ÅLCULO MANUAL DAS M√âTRICAS\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Accuracy:  {(tp + tn) / (tp + tn + fp + fn):.3f}\")\n",
    "print(f\"Precision: {tp / (tp + fp):.3f}\")\n",
    "print(f\"Recall:    {tp / (tp + fn):.3f}\")\n",
    "print(f\"Specificity: {tn / (tn + fp):.3f}\")  # Taxa de verdadeiros negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1486e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report - resumo completo\n",
    "print(\"üìã CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Classe 0\", \"Classe 1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e833e7",
   "metadata": {},
   "source": [
    "## 3. ROC Curve e AUC\n",
    "\n",
    "A **ROC Curve** (Receiver Operating Characteristic) plota a Taxa de Verdadeiros Positivos vs Taxa de Falsos Positivos para diferentes thresholds.\n",
    "\n",
    "A **AUC** (Area Under Curve) quantifica a qualidade do modelo: quanto maior (pr√≥ximo de 1), melhor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd170f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando ROC Curve e AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Plotando ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f\"ROC Curve (AUC = {auc_score:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"Random Classifier\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"Taxa de Falsos Positivos (1 - Specificity)\")\n",
    "plt.ylabel(\"Taxa de Verdadeiros Positivos (Sensitivity)\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"üéØ AUC Score: {auc_score:.3f}\")\n",
    "\n",
    "# Interpreta√ß√£o do AUC\n",
    "if auc_score >= 0.9:\n",
    "    interpretation = \"Excelente\"\n",
    "elif auc_score >= 0.8:\n",
    "    interpretation = \"Bom\"\n",
    "elif auc_score >= 0.7:\n",
    "    interpretation = \"Regular\"\n",
    "elif auc_score >= 0.6:\n",
    "    interpretation = \"Ruim\"\n",
    "else:\n",
    "    interpretation = \"Muito Ruim\"\n",
    "\n",
    "print(f\"üìä Interpreta√ß√£o: {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23cf41",
   "metadata": {},
   "source": [
    "## 4. Escolha da M√©trica Adequada\n",
    "\n",
    "A escolha da m√©trica depende do **contexto do problema** e do **custo dos erros**:\n",
    "\n",
    "### Quando usar cada m√©trica:\n",
    "\n",
    "**üéØ Accuracy**: Classes balanceadas e custos de erro similares\n",
    "\n",
    "**‚öñÔ∏è Precision**: Quando **False Positives s√£o custosos**\n",
    "\n",
    "- Ex: Diagn√≥stico m√©dico (evitar alarmes falsos)\n",
    "- Ex: Spam detection (evitar marcar emails leg√≠timos como spam)\n",
    "\n",
    "**üîç Recall**: Quando **False Negatives s√£o custosos**\n",
    "\n",
    "- Ex: Detec√ß√£o de fraude (n√£o pode deixar passar fraudes)\n",
    "- Ex: Diagn√≥stico de doen√ßas graves (n√£o pode deixar casos passarem)\n",
    "\n",
    "**üîÑ F1-Score**: Balanceamento entre Precision e Recall\n",
    "\n",
    "**üìà AUC-ROC**: Avalia√ß√£o geral, independente do threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194656e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo pr√°tico: Dataset desbalanceado\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Criando dataset desbalanceado (90% classe 0, 10% classe 1)\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.9, 0.1],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.3, random_state=42, stratify=y_imb\n",
    ")\n",
    "\n",
    "print(f\"Dataset Desbalanceado:\")\n",
    "print(f\"- Distribui√ß√£o treino: {np.bincount(y_train_imb)}\")\n",
    "print(f\"- Propor√ß√µes: {np.bincount(y_train_imb) / len(y_train_imb)}\")\n",
    "\n",
    "# Treinar modelo no dataset desbalanceado\n",
    "clf_imb = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "clf_imb.fit(X_train_imb, y_train_imb)\n",
    "\n",
    "# Predi√ß√µes\n",
    "y_pred_imb = clf_imb.predict(X_test_imb)\n",
    "y_proba_imb = clf_imb.predict_proba(X_test_imb)[:, 1]\n",
    "\n",
    "# Comparando m√©tricas\n",
    "print(\"\\nüìä COMPARA√á√ÉO DE M√âTRICAS - Dataset Desbalanceado\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test_imb, y_pred_imb):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test_imb, y_pred_imb):.3f}\")\n",
    "print(f\"Recall:    {recall_score(y_test_imb, y_pred_imb):.3f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test_imb, y_pred_imb):.3f}\")\n",
    "print(f\"AUC:       {roc_auc_score(y_test_imb, y_proba_imb):.3f}\")\n",
    "\n",
    "# Modelo \"ing√™nuo\" que sempre prediz classe majorit√°ria\n",
    "y_naive = np.zeros_like(y_test_imb)  # Sempre prediz classe 0\n",
    "accuracy_naive = accuracy_score(y_test_imb, y_naive)\n",
    "\n",
    "print(f\"\\nü§ñ Modelo 'ing√™nuo' (sempre classe majorit√°ria): {accuracy_naive:.3f}\")\n",
    "print(\"‚ö†Ô∏è  Isso mostra por que accuracy pode ser enganosa!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35491b",
   "metadata": {},
   "source": [
    "## 5. Resumo das M√©tricas\n",
    "\n",
    "### üìã Checklist para Escolha de M√©tricas\n",
    "\n",
    "1. **Analisar o dataset**:\n",
    "\n",
    "   - Classes balanceadas ou desbalanceadas?\n",
    "   - Qual o tamanho dos dados?\n",
    "\n",
    "2. **Entender o problema**:\n",
    "\n",
    "   - Qual o custo de um falso positivo?\n",
    "   - Qual o custo de um falso negativo?\n",
    "   - H√° prefer√™ncia por sensibilidade ou especificidade?\n",
    "\n",
    "3. **Selecionar m√©tricas apropriadas**:\n",
    "   - **Balanceado**: Accuracy, F1-Score, AUC\n",
    "   - **Desbalanceado**: Precision, Recall, F1-Score, AUC\n",
    "   - **Custo alto de FP**: Precision\n",
    "   - **Custo alto de FN**: Recall\n",
    "\n",
    "### üéØ Principais Takeaways\n",
    "\n",
    "- **Accuracy n√£o √© sempre a melhor m√©trica**\n",
    "- **Precision vs Recall √© um trade-off**\n",
    "- **F1-Score equilibra Precision e Recall**\n",
    "- **AUC √© √∫til para comparar modelos**\n",
    "- **A escolha depende do contexto do neg√≥cio**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a4499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara√ß√£o final de m√©tricas\n",
    "def evaluate_model(y_true, y_pred, y_proba, model_name=\"Modelo\"):\n",
    "    \"\"\"Fun√ß√£o utilit√°ria para avaliar um modelo com todas as m√©tricas\"\"\"\n",
    "\n",
    "    results = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred),\n",
    "        \"Recall\": recall_score(y_true, y_pred),\n",
    "        \"F1-Score\": f1_score(y_true, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_true, y_proba),\n",
    "    }\n",
    "\n",
    "    print(f\"üìä {model_name}\")\n",
    "    print(\"-\" * 30)\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric:10}: {value:.3f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Avaliando nosso modelo principal\n",
    "print(\"üèÜ AVALIA√á√ÉO FINAL\")\n",
    "print(\"=\" * 40)\n",
    "results = evaluate_model(y_test, y_pred, y_proba, \"Decision Tree\")\n",
    "\n",
    "print(f\"\\n‚úÖ Li√ß√£o 03 - M√©tricas de Avalia√ß√£o conclu√≠da!\")\n",
    "print(\"Pr√≥xima aula: Hyperparameter Tuning\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
