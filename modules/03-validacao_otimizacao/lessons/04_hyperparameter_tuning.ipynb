{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a677c5de",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Este notebook apresenta t√©cnicas para otimiza√ß√£o de hiperpar√¢metros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports b√°sicos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Configura√ß√£o de plotting\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0bb5b",
   "metadata": {},
   "source": [
    "## Objetivos da Aula\n",
    "\n",
    "- Grid Search\n",
    "- Random Search\n",
    "- Nested Cross-Validation\n",
    "- Estrat√©gias de otimiza√ß√£o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria√ß√£o de dataset sint√©tico para demonstra√ß√£o\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=10, n_redundant=0, n_informative=8, n_clusters_per_class=2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset criado:\")\n",
    "print(f\"- N√∫mero de amostras: {X.shape[0]}\")\n",
    "print(f\"- N√∫mero de features: {X.shape[1]}\")\n",
    "print(f\"- Distribui√ß√£o de classes: {np.bincount(y)}\")\n",
    "\n",
    "# Configurar seed para reprodutibilidade\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n‚úÖ Ambiente configurado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae25b9",
   "metadata": {},
   "source": [
    "## 1. O que s√£o Hiperpar√¢metros?\n",
    "\n",
    "**Hiperpar√¢metros** s√£o configura√ß√µes do modelo que **n√£o s√£o aprendidas** durante o treinamento, mas devem ser definidas **antes** do processo de aprendizado.\n",
    "\n",
    "### Exemplos de Hiperpar√¢metros:\n",
    "\n",
    "- **Random Forest**: `n_estimators`, `max_depth`, `min_samples_split`\n",
    "- **SVM**: `C`, `kernel`, `gamma`\n",
    "- **Neural Networks**: `learning_rate`, `batch_size`, `hidden_layers`\n",
    "\n",
    "### Por que otimizar?\n",
    "\n",
    "A escolha inadequada de hiperpar√¢metros pode levar a:\n",
    "\n",
    "- **Underfitting**: Modelo muito simples\n",
    "- **Overfitting**: Modelo muito complexo\n",
    "- **Performance sub√≥tima**: N√£o explorar o potencial m√°ximo do modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309445bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Impacto dos hiperpar√¢metros na performance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split dos dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Testando diferentes valores de max_depth no Random Forest\n",
    "depths = [3, 5, 10, 15, None]\n",
    "scores = []\n",
    "\n",
    "print(\"üå≥ Impacto do max_depth no Random Forest:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "for depth in depths:\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    score = accuracy_score(y_test, rf.predict(X_test))\n",
    "    scores.append(score)\n",
    "    print(f\"max_depth={str(depth):4} ‚Üí Accuracy: {score:.3f}\")\n",
    "\n",
    "# Visualiza√ß√£o do impacto\n",
    "plt.figure(figsize=(8, 5))\n",
    "depth_labels = [str(d) if d is not None else \"None\" for d in depths]\n",
    "plt.plot(depth_labels, scores, \"o-\", linewidth=2, markersize=8)\n",
    "plt.title(\"Impacto do max_depth na Performance\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Melhor max_depth: {depths[np.argmax(scores)]} (Accuracy: {max(scores):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7db46",
   "metadata": {},
   "source": [
    "## 2. Grid Search\n",
    "\n",
    "**Grid Search** √© uma t√©cnica que testa **todas as combina√ß√µes** poss√≠veis de hiperpar√¢metros em uma grade (grid) pr√©-definida.\n",
    "\n",
    "### Vantagens:\n",
    "\n",
    "- ‚úÖ Garante encontrar a melhor combina√ß√£o dentro da grade\n",
    "- ‚úÖ F√°cil de implementar e entender\n",
    "\n",
    "### Desvantagens:\n",
    "\n",
    "- ‚ùå Computacionalmente custoso (crescimento exponencial)\n",
    "- ‚ùå Limitado √†s combina√ß√µes pr√©-definidas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search com Random Forest\n",
    "import time\n",
    "\n",
    "# Definindo a grade de hiperpar√¢metros\n",
    "param_grid = {\"n_estimators\": [50, 100, 200], \"max_depth\": [5, 10, 15], \"min_samples_split\": [2, 5, 10]}\n",
    "\n",
    "print(\"üîç Grid Search - Random Forest\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Par√¢metros a testar: {param_grid}\")\n",
    "print(f\"Total de combina√ß√µes: {3 * 3 * 3} = 27\")\n",
    "\n",
    "# Grid Search com Cross-Validation\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,  # Usar todos os cores dispon√≠veis\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Tempo de execu√ß√£o: {end_time - start_time:.2f} segundos\")\n",
    "print(f\"üèÜ Melhor score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"üéØ Melhores par√¢metros: {grid_search.best_params_}\")\n",
    "\n",
    "# Avalia√ß√£o no conjunto de teste\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = accuracy_score(y_test, best_model.predict(X_test))\n",
    "print(f\"üìä Score no teste: {test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d68ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise dos resultados do Grid Search\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Top 5 combina√ß√µes\n",
    "print(\"üèÖ TOP 5 COMBINA√á√ïES\")\n",
    "print(\"=\" * 50)\n",
    "top_5 = results_df.nlargest(5, \"mean_test_score\")[[\"params\", \"mean_test_score\", \"std_test_score\"]]\n",
    "\n",
    "for i, (idx, row) in enumerate(top_5.iterrows()):\n",
    "    print(f\"{i+1}. Score: {row['mean_test_score']:.3f} (¬±{row['std_test_score']:.3f})\")\n",
    "    print(f\"   Params: {row['params']}\")\n",
    "    print()\n",
    "\n",
    "# Visualiza√ß√£o do impacto dos hiperpar√¢metros\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Impacto do n_estimators\n",
    "n_est_scores = results_df.groupby(\"param_n_estimators\")[\"mean_test_score\"].mean()\n",
    "axes[0].bar(range(len(n_est_scores)), n_est_scores.values)\n",
    "axes[0].set_title(\"Impacto do n_estimators\")\n",
    "axes[0].set_xlabel(\"n_estimators\")\n",
    "axes[0].set_ylabel(\"Mean CV Score\")\n",
    "axes[0].set_xticks(range(len(n_est_scores)))\n",
    "axes[0].set_xticklabels(n_est_scores.index)\n",
    "\n",
    "# Impacto do max_depth\n",
    "depth_scores = results_df.groupby(\"param_max_depth\")[\"mean_test_score\"].mean()\n",
    "axes[1].bar(range(len(depth_scores)), depth_scores.values)\n",
    "axes[1].set_title(\"Impacto do max_depth\")\n",
    "axes[1].set_xlabel(\"max_depth\")\n",
    "axes[1].set_ylabel(\"Mean CV Score\")\n",
    "axes[1].set_xticks(range(len(depth_scores)))\n",
    "axes[1].set_xticklabels(depth_scores.index)\n",
    "\n",
    "# Impacto do min_samples_split\n",
    "split_scores = results_df.groupby(\"param_min_samples_split\")[\"mean_test_score\"].mean()\n",
    "axes[2].bar(range(len(split_scores)), split_scores.values)\n",
    "axes[2].set_title(\"Impacto do min_samples_split\")\n",
    "axes[2].set_xlabel(\"min_samples_split\")\n",
    "axes[2].set_ylabel(\"Mean CV Score\")\n",
    "axes[2].set_xticks(range(len(split_scores)))\n",
    "axes[2].set_xticklabels(split_scores.index)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d2b67f",
   "metadata": {},
   "source": [
    "## 3. Random Search\n",
    "\n",
    "**Random Search** amostra **aleatoriamente** combina√ß√µes de hiperpar√¢metros de distribui√ß√µes definidas.\n",
    "\n",
    "### Vantagens:\n",
    "\n",
    "- ‚úÖ Mais eficiente que Grid Search para espa√ßos grandes\n",
    "- ‚úÖ Pode encontrar combina√ß√µes n√£o previstas\n",
    "- ‚úÖ Controle do tempo de execu√ß√£o (n√∫mero de itera√ß√µes)\n",
    "\n",
    "### Desvantagens:\n",
    "\n",
    "- ‚ùå N√£o garante encontrar o √≥timo global\n",
    "- ‚ùå Resultados podem variar entre execu√ß√µes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c5615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search com distribui√ß√µes cont√≠nuas\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Definindo distribui√ß√µes para os hiperpar√¢metros\n",
    "param_distributions = {\n",
    "    \"n_estimators\": randint(50, 300),  # Inteiros de 50 a 299\n",
    "    \"max_depth\": randint(3, 20),  # Inteiros de 3 a 19\n",
    "    \"min_samples_split\": randint(2, 20),  # Inteiros de 2 a 19\n",
    "    \"min_samples_leaf\": randint(1, 10),  # Inteiros de 1 a 9\n",
    "    \"max_features\": uniform(0.1, 0.9),  # Float de 0.1 a 1.0\n",
    "}\n",
    "\n",
    "print(\"üé≤ Random Search - Random Forest\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Distribui√ß√µes dos par√¢metros:\")\n",
    "for param, dist in param_distributions.items():\n",
    "    print(f\"- {param}: {dist}\")\n",
    "\n",
    "# Random Search\n",
    "start_time = time.time()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # 50 itera√ß√µes aleat√≥rias\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Tempo de execu√ß√£o: {end_time - start_time:.2f} segundos\")\n",
    "print(f\"üèÜ Melhor score: {random_search.best_score_:.3f}\")\n",
    "print(f\"üéØ Melhores par√¢metros: {random_search.best_params_}\")\n",
    "\n",
    "# Compara√ß√£o com Grid Search\n",
    "print(f\"\\nüìä COMPARA√á√ÉO\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Grid Search:   {grid_search.best_score_:.3f}\")\n",
    "print(f\"Random Search: {random_search.best_score_:.3f}\")\n",
    "\n",
    "# Avalia√ß√£o no teste\n",
    "random_test_score = accuracy_score(y_test, random_search.best_estimator_.predict(X_test))\n",
    "print(f\"\\nScore no teste:\")\n",
    "print(f\"Grid Search:   {test_score:.3f}\")\n",
    "print(f\"Random Search: {random_test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc23f353",
   "metadata": {},
   "source": [
    "## 4. Nested Cross-Validation\n",
    "\n",
    "**Nested CV** √© a forma **correta** de avaliar a performance de um modelo quando fazemos hyperparameter tuning.\n",
    "\n",
    "### Por que usar?\n",
    "\n",
    "- **Problema**: Usar o mesmo conjunto para tuning e avalia√ß√£o leva a **overfitting**\n",
    "- **Solu√ß√£o**: CV externo para avalia√ß√£o + CV interno para tuning\n",
    "\n",
    "### Estrutura:\n",
    "\n",
    "```\n",
    "Outer CV (avalia√ß√£o):\n",
    "‚îú‚îÄ‚îÄ Fold 1: Inner CV (tuning) ‚Üí Melhor modelo ‚Üí Avalia√ß√£o\n",
    "‚îú‚îÄ‚îÄ Fold 2: Inner CV (tuning) ‚Üí Melhor modelo ‚Üí Avalia√ß√£o\n",
    "‚îî‚îÄ‚îÄ Fold 3: Inner CV (tuning) ‚Üí Melhor modelo ‚Üí Avalia√ß√£o\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48478872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementando Nested Cross-Validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Configura√ß√£o dos CVs\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid mais simples para demonstra√ß√£o\n",
    "simple_param_grid = {\"n_estimators\": [50, 100], \"max_depth\": [5, 10, None]}\n",
    "\n",
    "print(\"üîÑ Nested Cross-Validation\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Outer CV: {outer_cv.n_splits} folds\")\n",
    "print(f\"Inner CV: {inner_cv.n_splits} folds\")\n",
    "\n",
    "# Implementa√ß√£o manual do Nested CV\n",
    "nested_scores = []\n",
    "best_params_per_fold = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(outer_cv.split(X, y)):\n",
    "    print(f\"\\nüìÅ Outer Fold {fold + 1}\")\n",
    "\n",
    "    # Split dos dados do outer fold\n",
    "    X_train_outer, X_val_outer = X[train_idx], X[val_idx]\n",
    "    y_train_outer, y_val_outer = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Inner CV para hyperparameter tuning\n",
    "    inner_grid = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42), simple_param_grid, cv=inner_cv, scoring=\"accuracy\"\n",
    "    )\n",
    "\n",
    "    inner_grid.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # Avalia√ß√£o no validation set do outer fold\n",
    "    best_model = inner_grid.best_estimator_\n",
    "    score = accuracy_score(y_val_outer, best_model.predict(X_val_outer))\n",
    "\n",
    "    nested_scores.append(score)\n",
    "    best_params_per_fold.append(inner_grid.best_params_)\n",
    "\n",
    "    print(f\"   Melhores params: {inner_grid.best_params_}\")\n",
    "    print(f\"   Score: {score:.3f}\")\n",
    "\n",
    "# Resultado final\n",
    "mean_score = np.mean(nested_scores)\n",
    "std_score = np.std(nested_scores)\n",
    "\n",
    "print(f\"\\nüéØ RESULTADO FINAL\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Nested CV Score: {mean_score:.3f} ¬± {std_score:.3f}\")\n",
    "print(f\"Scores por fold: {[f'{s:.3f}' for s in nested_scores]}\")\n",
    "\n",
    "# Compara√ß√£o com valida√ß√£o \"ing√™nua\"\n",
    "naive_score = grid_search.best_score_\n",
    "print(f\"\\nüìä COMPARA√á√ÉO\")\n",
    "print(\"=\" * 20)\n",
    "print(f\"Valida√ß√£o 'ing√™nua': {naive_score:.3f}\")\n",
    "print(f\"Nested CV:          {mean_score:.3f} ¬± {std_score:.3f}\")\n",
    "print(\"\\n‚ö†Ô∏è  A diferen√ßa mostra o overfitting do tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f978ec0",
   "metadata": {},
   "source": [
    "## 5. Estrat√©gias Pr√°ticas de Otimiza√ß√£o\n",
    "\n",
    "### üéØ Boas Pr√°ticas:\n",
    "\n",
    "1. **Comece simples**: Teste ranges amplos primeiro\n",
    "2. **Use Random Search**: Para explora√ß√£o inicial\n",
    "3. **Refine com Grid Search**: Em regi√µes promissoras\n",
    "4. **Monitore overfitting**: Use Nested CV para avalia√ß√£o final\n",
    "5. **Considere o custo computacional**: Balance precis√£o vs tempo\n",
    "\n",
    "### üìä Quando usar cada m√©todo:\n",
    "\n",
    "| M√©todo                    | Situa√ß√£o                  | Vantagem            |\n",
    "| ------------------------- | ------------------------- | ------------------- |\n",
    "| **Manual**                | Poucos hiperpar√¢metros    | Controle total      |\n",
    "| **Grid Search**           | Espa√ßo pequeno e discreto | Garantia de √≥timo   |\n",
    "| **Random Search**         | Espa√ßo grande ou cont√≠nuo | Efici√™ncia          |\n",
    "| **Bayesian Optimization** | Fun√ß√£o cara de avaliar    | Menor n¬∞ avalia√ß√µes |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Estrat√©gia h√≠brida (Random + Grid)\n",
    "print(\"üöÄ ESTRAT√âGIA H√çBRIDA\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Passo 1: Random Search para explora√ß√£o ampla\n",
    "print(\"Passo 1: Explora√ß√£o com Random Search...\")\n",
    "\n",
    "broad_distributions = {\n",
    "    \"n_estimators\": randint(10, 500),\n",
    "    \"max_depth\": randint(1, 30),\n",
    "    \"min_samples_split\": randint(2, 50),\n",
    "    \"min_samples_leaf\": randint(1, 20),\n",
    "}\n",
    "\n",
    "# Random search com muitas itera√ß√µes\n",
    "exploration = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    broad_distributions,\n",
    "    n_iter=30,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "exploration.fit(X_train, y_train)\n",
    "best_broad = exploration.best_params_\n",
    "\n",
    "print(f\"Melhor regi√£o encontrada: {best_broad}\")\n",
    "\n",
    "# Passo 2: Grid Search refinado na regi√£o promissora\n",
    "print(\"\\nPasso 2: Refinamento com Grid Search...\")\n",
    "\n",
    "# Definir grid ao redor dos melhores valores\n",
    "refined_grid = {\n",
    "    \"n_estimators\": [\n",
    "        max(10, best_broad[\"n_estimators\"] - 50),\n",
    "        best_broad[\"n_estimators\"],\n",
    "        best_broad[\"n_estimators\"] + 50,\n",
    "    ],\n",
    "    \"max_depth\": [max(1, best_broad[\"max_depth\"] - 2), best_broad[\"max_depth\"], best_broad[\"max_depth\"] + 2],\n",
    "    \"min_samples_split\": [\n",
    "        max(2, best_broad[\"min_samples_split\"] - 2),\n",
    "        best_broad[\"min_samples_split\"],\n",
    "        min(50, best_broad[\"min_samples_split\"] + 2),\n",
    "    ],\n",
    "}\n",
    "\n",
    "refinement = GridSearchCV(RandomForestClassifier(random_state=42), refined_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "refinement.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores par√¢metros refinados: {refinement.best_params_}\")\n",
    "print(f\"Score final: {refinement.best_score_:.3f}\")\n",
    "\n",
    "# Compara√ß√£o final\n",
    "final_test_score = accuracy_score(y_test, refinement.best_estimator_.predict(X_test))\n",
    "\n",
    "print(f\"\\nüìà COMPARA√á√ÉO FINAL NO TESTE\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Grid Search simples:    {test_score:.3f}\")\n",
    "print(f\"Random Search:          {random_test_score:.3f}\")\n",
    "print(f\"Estrat√©gia h√≠brida:     {final_test_score:.3f}\")\n",
    "\n",
    "print(\n",
    "    f\"\\nüéØ Melhor abordagem: {'H√≠brida' if final_test_score == max(test_score, random_test_score, final_test_score) else 'Outra'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a8782",
   "metadata": {},
   "source": [
    "## 6. Resumo e Boas Pr√°ticas\n",
    "\n",
    "### üéØ Principais Takeaways:\n",
    "\n",
    "1. **Hiperpar√¢metros s√£o cruciais** para performance dos modelos\n",
    "2. **Grid Search**: Exaustivo mas garantido (espa√ßos pequenos)\n",
    "3. **Random Search**: Eficiente para explora√ß√£o (espa√ßos grandes)\n",
    "4. **Nested CV**: Avalia√ß√£o n√£o enviesada com tuning\n",
    "5. **Estrat√©gia h√≠brida**: Explora√ß√£o + refinamento\n",
    "\n",
    "### ‚ö†Ô∏è Armadilhas Comuns:\n",
    "\n",
    "- **Data leakage**: N√£o usar dados de teste para tuning\n",
    "- **Overfitting do tuning**: Muitas itera√ß√µes sem valida√ß√£o externa\n",
    "- **Ignorar o custo computacional**: Balance efici√™ncia vs precis√£o\n",
    "- **Grid muito denso**: Pode n√£o melhorar significativamente\n",
    "\n",
    "### üöÄ Pr√≥ximos Passos:\n",
    "\n",
    "- **Bayesian Optimization**: Para otimiza√ß√£o mais inteligente\n",
    "- **Multi-objective optimization**: Balance entre m√©tricas\n",
    "- **Early stopping**: Para modelos iterativos\n",
    "- **AutoML**: Automatiza√ß√£o completa do processo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7cd2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o utilit√°ria para hyperparameter tuning\n",
    "def optimize_hyperparameters(\n",
    "    estimator, param_grid, X, y, method=\"grid\", n_iter=50, cv=5, scoring=\"accuracy\", random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o utilit√°ria para otimiza√ß√£o de hiperpar√¢metros\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    estimator : sklearn estimator\n",
    "        Modelo a ser otimizado\n",
    "    param_grid : dict\n",
    "        Grade ou distribui√ß√µes de par√¢metros\n",
    "    X, y : array-like\n",
    "        Dados de treino\n",
    "    method : str, default='grid'\n",
    "        M√©todo de busca ('grid' ou 'random')\n",
    "    n_iter : int, default=50\n",
    "        N√∫mero de itera√ß√µes para random search\n",
    "    cv : int, default=5\n",
    "        N√∫mero de folds para cross-validation\n",
    "    scoring : str, default='accuracy'\n",
    "        M√©trica de avalia√ß√£o\n",
    "    random_state : int, default=42\n",
    "        Seed para reprodutibilidade\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Best estimator fitted\n",
    "    \"\"\"\n",
    "\n",
    "    if method == \"grid\":\n",
    "        search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    elif method == \"random\":\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"method deve ser 'grid' ou 'random'\")\n",
    "\n",
    "    search.fit(X, y)\n",
    "\n",
    "    print(f\"üèÜ Melhor score ({method}): {search.best_score_:.3f}\")\n",
    "    print(f\"üéØ Melhores par√¢metros: {search.best_params_}\")\n",
    "\n",
    "    return search.best_estimator_\n",
    "\n",
    "\n",
    "# Exemplo de uso da fun√ß√£o\n",
    "print(\"üõ†Ô∏è FUN√á√ÉO UTILIT√ÅRIA\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Teste r√°pido\n",
    "quick_grid = {\"n_estimators\": [50, 100], \"max_depth\": [5, 10]}\n",
    "best_rf = optimize_hyperparameters(\n",
    "    RandomForestClassifier(random_state=42), quick_grid, X_train, y_train, method=\"grid\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Li√ß√£o 04 - Hyperparameter Tuning conclu√≠da!\")\n",
    "print(\"M√≥dulo de Valida√ß√£o e Otimiza√ß√£o finalizado!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
