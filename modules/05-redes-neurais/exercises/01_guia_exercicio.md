# Guia do Exerc√≠cio 01: Classifica√ß√£o de D√≠gitos MNIST

## Contexto Acad√™mico

O dataset MNIST (Modified National Institute of Standards and Technology) √© um dos datasets mais ic√¥nicos em Machine Learning, sendo considerado o "Hello World" das redes neurais. Este exerc√≠cio introduz conceitos fundamentais de redes neurais Multi-Layer Perceptron (MLP) aplicados a um problema real de classifica√ß√£o de imagens.

## Objetivos de Aprendizagem

Ao concluir este exerc√≠cio, voc√™ ser√° capaz de:

1. **Carregar e explorar** datasets de imagens de d√≠gitos manuscritos
2. **Pr√©-processar dados** de imagem para redes neurais
3. **Implementar e treinar** redes neurais MLP usando scikit-learn
4. **Otimizar hiperpar√¢metros** de arquitetura da rede
5. **Avaliar performance** usando m√©tricas apropriadas para classifica√ß√£o
6. **Analisar erros** e interpretar predi√ß√µes do modelo
7. **Aplicar regulariza√ß√£o** para melhorar generaliza√ß√£o

## Fundamenta√ß√£o Te√≥rica

### 1. O Dataset MNIST

**Caracter√≠sticas:**

- 70.000 imagens de d√≠gitos manuscritos (0-9)
- 60.000 para treinamento, 10.000 para teste
- Imagens em escala de cinza de 28√ó28 pixels
- 784 features (28 √ó 28 = 784 pixels achatados)
- 10 classes balanceadas (um d√≠gito por classe)

**Hist√≥rico:**
Criado em 1998 por Yann LeCun, √© amplamente usado como benchmark para algoritmos de classifica√ß√£o de imagens.

### 2. Redes Neurais Multi-Layer Perceptron (MLP)

**Arquitetura:**

```
Input Layer (784) ‚Üí Hidden Layer(s) ‚Üí Output Layer (10)
```

**Componentes:**

- **Camada de Entrada**: 784 neur√¥nios (um por pixel)
- **Camadas Ocultas**: N√∫mero vari√°vel de neur√¥nios e camadas
- **Camada de Sa√≠da**: 10 neur√¥nios (um por classe)

**Fun√ß√£o de Ativa√ß√£o ReLU:**

```
ReLU(x) = max(0, x)
```

- Introduz n√£o-linearidade
- Evita problema de gradiente desvanecente
- Computacionalmente eficiente

### 3. Pr√©-processamento de Imagens

#### 3.1 Normaliza√ß√£o de Pixels

**Por que normalizar?**

- Pixels originais: valores de 0 a 255
- Redes neurais convergem melhor com valores pequenos
- Evita domin√¢ncia de features com valores grandes

**T√©cnica comum para imagens:**

```python
X_normalized = X / 255.0  # Range [0, 1]
```

**Alternativa: StandardScaler**

```python
X_scaled = (X - mean) / std_dev
```

### 4. Otimiza√ß√£o de Hiperpar√¢metros

#### 4.1 Arquitetura da Rede

**Hiperpar√¢metros principais:**

| Par√¢metro            | Descri√ß√£o                           | Valores T√≠picos            |
| -------------------- | ----------------------------------- | -------------------------- |
| `hidden_layer_sizes` | Tamanho e n√∫mero de camadas ocultas | (50,), (100,), (100, 50)   |
| `activation`         | Fun√ß√£o de ativa√ß√£o                  | 'relu', 'tanh', 'logistic' |
| `max_iter`           | N√∫mero m√°ximo de itera√ß√µes          | 10-100                     |
| `alpha`              | Par√¢metro de regulariza√ß√£o L2       | 0.0001, 0.001, 0.01        |
| `learning_rate_init` | Taxa de aprendizado inicial         | 0.001, 0.01, 0.1           |

**Trade-offs:**

- **Redes mais profundas**: Maior capacidade, mais tempo de treino, risco de overfitting
- **Redes mais rasas**: Treinamento r√°pido, menor capacidade, risco de underfitting

### 5. Regulariza√ß√£o

**Objetivo:** Prevenir overfitting, melhorar generaliza√ß√£o

**L2 Regularization (Ridge):**

```
Loss = Original_Loss + Œ± √ó Œ£(weights¬≤)
```

**Efeito:**

- Penaliza pesos grandes
- For√ßa o modelo a ser mais simples
- Alpha controla a intensidade da regulariza√ß√£o

**Valores de Alpha:**

- **Muito baixo (0.0001)**: Pouca regulariza√ß√£o, pode overfitar
- **Moderado (0.001-0.01)**: Bom equil√≠brio geralmente
- **Alto (0.1+)**: Muita regulariza√ß√£o, pode underfitar

### 6. M√©tricas de Avalia√ß√£o

#### 6.1 Acur√°cia

```
Accuracy = (Predi√ß√µes Corretas) / (Total de Predi√ß√µes)
```

**Quando usar:** Datasets balanceados (como MNIST)

#### 6.2 Matriz de Confus√£o

Mostra confus√µes entre classes:

```
           Previsto
           0  1  2  ...  9
Real   0 [95  0  1  ...  0]
       1 [ 0 98  0  ...  0]
       2 [ 1  0 94  ...  0]
       ...
       9 [ 0  0  0  ... 97]
```

**Interpreta√ß√£o:**

- Diagonal principal: predi√ß√µes corretas
- Fora da diagonal: confus√µes entre classes
- C√©lulas com valores altos indicam d√≠gitos similares

#### 6.3 Classification Report

Fornece m√©tricas por classe:

- **Precision**: Quando prev√™ X, quantas vezes est√° correto?
- **Recall**: De todos os X verdadeiros, quantos foram encontrados?
- **F1-Score**: M√©dia harm√¥nica de precision e recall

## Implementa√ß√£o Pr√°tica

### Fluxo do Exerc√≠cio

```
1. Carregar MNIST
   ‚Üì
2. Explorar dados (visualiza√ß√µes, distribui√ß√£o)
   ‚Üì
3. Preparar dados (split, normaliza√ß√£o)
   ‚Üì
4. Treinar MLP b√°sico
   ‚Üì
5. Otimizar arquitetura
   ‚Üì
6. Analisar performance (matriz de confus√£o)
   ‚Üì
7. An√°lise de erros
   ‚Üì
8. Experimentar regulariza√ß√£o
   ‚Üì
9. Conclus√µes
```

### Dicas de Implementa√ß√£o

#### Tarefa 1: Carregamento de Dados

```python
from sklearn.datasets import fetch_openml

# Carregar MNIST
mnist = fetch_openml('mnist_784', version=1, parser='auto')
X = mnist.data
y = mnist.target.astype(int)

# Visualizar imagem
plt.imshow(X[0].reshape(28, 28), cmap='gray')
```

#### Tarefa 2: Prepara√ß√£o

```python
# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Normalizar
X_train_norm = X_train / 255.0
X_test_norm = X_test / 255.0
```

#### Tarefa 3: MLP B√°sico

```python
from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(
    hidden_layer_sizes=(100,),
    activation='relu',
    max_iter=20,
    random_state=42,
    verbose=True
)

mlp.fit(X_train_norm, y_train)
```

#### Tarefa 4: Compara√ß√£o de Arquiteturas

```python
arquiteturas = [(50,), (100,), (100, 50), (200, 100, 50)]

for arch in arquiteturas:
    mlp = MLPClassifier(hidden_layer_sizes=arch, ...)
    mlp.fit(X_train_norm, y_train)
    acc = mlp.score(X_test_norm, y_test)
    print(f"{arch}: {acc:.4f}")
```

#### Tarefa 5: Matriz de Confus√£o

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
```

#### Tarefa 6: An√°lise de Erros

```python
# Encontrar erros
erros_idx = np.where(y_test != y_pred)[0]

# Visualizar
for idx in erros_idx[:10]:
    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')
    plt.title(f"Real: {y_test[idx]}, Pred: {y_pred[idx]}")
```

#### Tarefa 7: Regulariza√ß√£o

```python
alphas = [0.0001, 0.001, 0.01, 0.1]

for alpha in alphas:
    mlp = MLPClassifier(alpha=alpha, ...)
    mlp.fit(X_train_norm, y_train)
    # Avaliar treino e teste
```

## Resultados Esperados

### Performance T√≠pica

- **MLP B√°sico (100,)**: ~95-96% de acur√°cia
- **MLP Otimizado (100, 50)**: ~97-98% de acur√°cia
- **State-of-the-art (CNNs)**: >99% de acur√°cia

### Confus√µes Comuns

D√≠gitos frequentemente confundidos:

- **4 ‚Üî 9**: Tra√ßos verticais similares
- **3 ‚Üî 5**: Curvaturas parecidas
- **7 ‚Üî 1**: Ambos t√™m tra√ßo vertical
- **8 ‚Üî 3**: M√∫ltiplas curvas

## Conceitos Avan√ßados (Opcional)

### Por que MLPs n√£o s√£o ideais para imagens?

**Limita√ß√µes:**

1. **Perda de estrutura espacial**: Achatamento destr√≥i rela√ß√µes espaciais 2D
2. **Muitos par√¢metros**: 784 √ó 100 = 78.400 pesos s√≥ na primeira camada
3. **N√£o invariante a transla√ß√£o**: Mesma imagem deslocada √© tratada diferente

**Solu√ß√£o:** Convolutional Neural Networks (CNNs)

- Preservam estrutura espacial
- Compartilham pesos (menos par√¢metros)
- Invariantes a transla√ß√£o

### Early Stopping

```python
mlp = MLPClassifier(
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=5
)
```

## Crit√©rios de Avalia√ß√£o

O exerc√≠cio ser√° avaliado considerando:

1. **Carregamento e explora√ß√£o** (15 pontos)

   - Dados carregados corretamente
   - Visualiza√ß√µes apropriadas
   - An√°lise da distribui√ß√£o

2. **Prepara√ß√£o dos dados** (15 pontos)

   - Split correto (80/20)
   - Normaliza√ß√£o adequada
   - Verifica√ß√£o de shapes

3. **MLP B√°sico** (20 pontos)

   - Modelo treinado
   - Acur√°cia > 80%
   - Classification report

4. **Otimiza√ß√£o de arquitetura** (25 pontos)

   - Pelo menos 3 arquiteturas testadas
   - Compara√ß√£o sistem√°tica
   - Escolha justificada

5. **Matriz de confus√£o** (15 pontos)

   - Matriz gerada corretamente
   - Visualiza√ß√£o clara
   - An√°lise de confus√µes

6. **An√°lise de erros** (20 pontos)

   - Exemplos de erros visualizados
   - Interpreta√ß√£o dos erros
   - Insights sobre padr√µes

7. **Regulariza√ß√£o** (20 pontos)

   - M√∫ltiplos valores de alpha testados
   - Compara√ß√£o treino vs teste
   - An√°lise do impacto

8. **Conclus√µes** (10 pontos)
   - Resumo dos resultados
   - Insights relevantes
   - Reflex√£o cr√≠tica

**Total: 140 pontos**

## Recursos Adicionais

### Documenta√ß√£o

- [scikit-learn MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)
- [MNIST Database](http://yann.lecun.com/exdb/mnist/)

### Leituras Recomendadas

- LeCun et al. (1998) - "Gradient-Based Learning Applied to Document Recognition"
- Goodfellow et al. (2016) - "Deep Learning" (Cap√≠tulo 6: Deep Feedforward Networks)

### Pr√≥ximos Passos

Ap√≥s dominar este exerc√≠cio, explore:

1. Implementar MLP do zero (sem scikit-learn)
2. Testar outras fun√ß√µes de ativa√ß√£o
3. Implementar CNN para MNIST
4. Aplicar Data Augmentation
5. Tentar Fashion-MNIST (dataset mais desafiador)

## D√∫vidas Frequentes

**Q: Por que usar apenas 10.000 amostras se temos 70.000?**  
A: Para acelerar o treinamento durante o aprendizado. Use o dataset completo para resultados finais.

**Q: Qual a diferen√ßa entre max_iter e epochs?**  
A: No scikit-learn, max_iter √© o n√∫mero m√°ximo de itera√ß√µes do otimizador, similar a epochs em frameworks como TensorFlow.

**Q: Por que normalizar para [0,1] e n√£o usar StandardScaler?**  
A: Ambos funcionam! Para imagens, [0,1] √© mais intuitivo (representa intensidade), mas StandardScaler tamb√©m √© v√°lido.

**Q: Como escolher o n√∫mero de camadas ocultas?**  
A: Comece com 1-2 camadas. Adicione mais se o modelo underfitta. Use regulariza√ß√£o se overfitta.

**Q: Qual a diferen√ßa entre MLP e Deep Learning?**  
A: MLP √© um tipo de rede neural. Deep Learning refere-se a redes neurais profundas (muitas camadas), geralmente CNNs, RNNs, etc.

---

**Boa sorte no exerc√≠cio! üöÄ**
